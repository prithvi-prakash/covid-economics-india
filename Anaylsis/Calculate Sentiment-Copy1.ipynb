{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f169ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d606167f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d88c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    " \n",
    " \n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Tasks:\n",
    "# emoji, emotion, hate, irony, offensive, sentiment\n",
    "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b67072e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentimentScores(text):\n",
    "\n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "#     scores = {}\n",
    "    sentiments = {}\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = labels[ranking[i]]\n",
    "        s = scores[ranking[i]]\n",
    "#         print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "        sentiments[l] = np.round(float(s), 4)\n",
    "    return sentiments\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9d57ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./bangalore/*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e467b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList = sorted(glob.glob(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7b9b65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File #1339/1339; Tweet #99/99\n"
     ]
    }
   ],
   "source": [
    "offset = 319\n",
    "for jdx, file in enumerate(fileList[offset:]):\n",
    "    sentiments = []\n",
    "    df = pd.read_csv(file)\n",
    "    for idx, tweet in enumerate(df[\"tweet\"]):\n",
    "        clear_output(wait=True)\n",
    "    #     print(df[\"index\"])\n",
    "        print(f\"File #{offset+jdx+1}/{len(fileList)}; Tweet #{idx+1}/{df.shape[0]}\")\n",
    "        try:\n",
    "            sentiments.append(getSentimentScores(tweet))\n",
    "        except:\n",
    "            pass\n",
    "    sent_df = df.join(pd.DataFrame(sentiments))\n",
    "    sent_df.to_csv(f\"{file.replace('bangalore', 'bangalore-sentiment')}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b813a16d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d695a7e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de332c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
